---
position: 2212
title: Ai Security Risk Scenarios
---

## AI Security Risk Scenarios

These five realistic scenarios are based on actual incidents that highlight common AI security risks business professionals might encounter. Each scenario is presented with discussion questions that prompt participants to identify security risks, assess potential consequences, propose appropriate mitigation strategies, and determine the proper escalation paths within their organization.

Scenario 1: The Innocent Data Share

Setting: The marketing department collaborating with an external agency

Situation: A marketing manager shares customer purchase history data with an external AI analytics firm to enhance targeted marketing. The data is exported to a spreadsheet and emailed without encryption or data minimization.

Real-life basis: Similar to the Marriott data breach where unencrypted customer data was inappropriately shared and accessed by unauthorized parties, resulting in a Â£18.4 million fine under GDPR.



Risk identification exercise: Participants identify issues related to unsecured data transmission, excessive data sharing, and inadequate vendor security assessments.

Scenario 2: The Conversational AI Overshare

Setting: Customer service department using AI chatbots

Situation: A team member uploads customer conversation logs to a public AI tool to analyze sentiment patterns, unaware that the conversations contain personal identifying information and that the tool uses these uploads to train its models.

Real-life basis: Inspired by incidents like Samsung's accidental exposure of internal data when employees used ChatGPT for code debugging and similar incidents at major banks where sensitive information was shared with public AI tools.

Risk identification exercise: Participants spot the improper use of public AI tools for sensitive data and the implications of training data retention.

Scenario 3: The Helpful Shortcut Setting: HR recruitment process

Situation: An HR manager bypasses the standard AI-powered applicant screening system by downloading resumes to a personal AI tool for faster analysis, as the company's system is deemed "too slow."

Real-life basis: Similar to the Thomson Reuters employee data leak where HR data was processed outside approved systems, exposing employee information.

Risk identification exercise: Participants identify shadow IT risks, unauthorized processing, and potential bias introduction when circumventing vetted systems.

Scenario 4: The Predictive Maintenance Mishap

Setting: Operations department using IoT sensors and AI

Situation: An operations team connects factory equipment sensors to a new AI predictive maintenance platform using default passwords and unrestricted network access.

Real-life basis: Similar to the 2021 Florida water treatment facility incident where weak security controls allowed access to critical systems, though in that case, it was direct hacking rather than through an AI system.

Risk identification exercise: Participants spot inadequate access controls, poor authentication practices, and lack of network segmentation.



Scenario 5: The Well-Intentioned Model Adjustment

Setting: The finance department uses an AI fraud detection system

Situation: After several legitimate transactions are flagged as suspicious, a finance team member repeatedly overrides the AI system's decisions without documenting reasons, effectively "training" the model to ignore important risk patterns.

Real-life basis: Reflects elements of the Capital One breach where improper model training and overrides led to security vulnerabilities that were exploited, resulting in a massive data breach affecting over 100 million customers.

Risk identification exercise: Participants identify risks associated with undocumented AI model adjustments, failure to retrain the system properly, and inadequate oversight of algorithm modifications.