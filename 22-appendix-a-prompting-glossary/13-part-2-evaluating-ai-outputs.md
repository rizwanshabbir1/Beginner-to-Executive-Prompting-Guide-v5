---
position: 2213
title: Part 2 Evaluating Ai Outputs
---

## Part 2: Evaluating AI Outputs

UNDERSTANDING AI OUTPUTS



Before we discuss how to evaluate AI, let's briefly understand how these systems generate responses.

Modern AI, including huge language models, is essentially a sophisticated

pattern-matching system. They've been trained on vast amounts of text from books, websites, and documents. When you ask a question, the AI identifies patterns from its training and generates a response that statistically aligns with what it has learned.

This approach has two important implications. First, AI doesn't truly "understand" information the way humans do. It lacks common sense and real-world experience. Second, AI doesn't necessarily distinguish between fact and fiction—it reproduces patterns it's seen, whether those patterns represent accurate information or not.

This leads to what is commonly called "hallucinations"—instances where AI confidently presents incorrect information as fact. Imagine asking a colleague a question, and they make up an answer rather than saying, "I don't know." That's essentially what AI hallucination appears to be.

Many AI systems are also "black boxes,"—meaning we can see what goes in and what comes out, but we can't fully observe the reasoning process in between. Think of it like



getting advice from someone who can't explain their rationale. The advice might be good, but without understanding the "why," it's difficult to judge its reliability.

In business settings, these limitations matter. If you're using AI to draft communications, analyze data, or support decisions, incorrect outputs could lead to embarrassment at best or significant business impacts at worst.

PRACTICAL AI EVALUATION TECHNIQUES



So, how can you evaluate AI outputs without becoming a technical expert? Here are five practical techniques:

First, cross-reference critical information. If an AI provides statistics, facts, or quotes, verify these with trusted sources before using them in essential contexts. This is especially crucial for time-sensitive information or industry-specific data. Remember, many AI systems have knowledge cutoff dates and won't be aware of recent developments.

Second, check for consistency. Try asking the same question multiple ways or break complex questions into smaller parts. If you receive contradictory answers, that's a red flag indicating the AI might be uncertain or experiencing hallucinations. Inconsistency often shows areas where you should be particularly skeptical.

Third, be wary of responses that seem too perfect. If an AI provides a highly comprehensive answer with numerous specific details, especially about obscure topics, proceed with caution. Real expertise often acknowledges complexity and limitations, while hallucinating AI might generate impressively detailed but fictional content.

Fourth, test boundaries and edge cases. If you're exploring a topic, ask about exceptions, contradictions, or alternative viewpoints. How the AI handles these challenges can reveal its limitations. A high-quality AI will acknowledge uncertainties and varying perspectives rather than presenting oversimplified views.

Fifth, develop a sense of outdated information. Suppose an AI refers to "recent events" from two years ago or mentions technology or policies that have since changed. In that case, that's a sign that you need to verify the currency of all information provided.



Here's a question to consider: When was the last time you received information from AI that you later discovered was incorrect? How might applying these techniques have led to a different outcome?